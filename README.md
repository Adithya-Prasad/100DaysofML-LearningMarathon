# 100 Days of ML: Join me for a learning marathon!

*I always wanted to take up some challenging work to get organized, and I found a way! I'll be writing articles and building projects over the next 100 days on Machine Learning. I’ll try to cover as many topics as possible, making them simple so everyone can follow along. If you feel like joining me for this marathon, feel free to jump in!*

I'm posting the roadmap for this 100-day journey here so we can stay on track with our goals.


![Designer (3)](https://github.com/user-attachments/assets/e1e8f9d5-c656-4915-afa0-7cfbd3fc8a8f)

### **Machine Learning 100-Day Roadmap**

#### **Week 1: Introduction to Machine Learning and Basics**
  **Day 1**: *What is Machine Learning?*—Types (Supervised, Unsupervised, Reinforcement), Applications.
  **Day 2**: *Understanding Data in ML*—Introduction to datasets, data structures, and data cleaning.
  **Day 3**: *Statistics Fundamentals*—Mean, Median, Mode, Variance, Standard Deviation.
  **Day 4**: *Probability Basics*—Basic probability, conditional probability, Bayes’ theorem.
  **Day 5**: *Exploratory Data Analysis (EDA)*—Visualization tools (Matplotlib, Seaborn) and techniques.
  **Day 6**: *Data Preprocessing Techniques*—Normalization, Standardization, Encoding.
  **Day 7**: *Feature Engineering Basics*—Feature selection and extraction.

#### **Week 2: Linear Models and Supervised Learning Basics**
  **Day 8**: *Linear Regression Theory*—Introduction to regression, linear regression theory.
  **Day 9**: *Linear Regression in Practice*—Implementing linear regression from scratch.
  **Day 10**: *Evaluation Metrics for Regression*—MSE, MAE, RMSE, R² Score.
  **Day 11**: *Gradient Descent*—Understanding the gradient descent algorithm.
  **Day 12**: *Logistic Regression Theory*—Binary classification, sigmoid function.
  **Day 13**: *Logistic Regression in Practice*—Implementing logistic regression from scratch.
  **Day 14**: *Evaluation Metrics for Classification*—Precision, Recall, F1-Score, ROC-AUC.

#### **Week 3: Supervised Learning - Advanced Algorithms**
  **Day 15**: *Decision Trees*—Theory and working of decision trees.
  **Day 16**: *Random Forests*—Introduction, how they improve decision trees.
  **Day 17**: *Bagging and Boosting*—Understanding ensemble techniques.
  **Day 18**: *AdaBoost and Gradient Boosting*—Theory and applications.
  **Day 19**: *XGBoost and CatBoost*—Optimized gradient boosting models.
  **Day 20**: *Support Vector Machines (SVM)*—Theory and kernel functions.
  **Day 21**: *Hyperparameter Tuning Basics*—Grid search, random search.

#### **Week 4: Unsupervised Learning — Clustering and Dimensionality Reduction**
  **Day 22**: Intro to Unsupervised Learning — Differences and common uses.
  **Day 23**: K-Means Clustering — Theory and implementation.
  **Day 24**: Hierarchical Clustering — Theory and applications.
  **Day 25**: DBSCAN and Density-Based Clustering — Non-linear clustering.
  **Day 26**: Principal Component Analysis (PCA) — Dimensionality reduction.
  **Day 27**: t-SNE and UMAP — Non-linear dimensionality reduction techniques.
  **Day 28**: Applications of Clustering — Customer segmentation and anomaly detection.

#### **Week 5: Feature Engineering and Data Challenges**
  **Day 29**: Feature Scaling — When to use normalization vs. standardization.
  **Day 30**: Encoding Categorical Variables — One-hot, label, and target encoding.
  **Day 31**: Handling Imbalanced Data — Oversampling, undersampling, SMOTE.
  **Day 32**: Outlier Detection and Removal — Techniques for handling outliers.
  **Day 33**: Feature Selection Techniques — Filter, wrapper, and embedded methods.
  **Day 34**: Data Leakage — Causes and prevention.
  **Day 35**: Time-Series Data Preprocessing — Handling temporal data.

#### **Week 6: Advanced Supervised Learning Techniques**
  **Day 36**: K-Nearest Neighbors (KNN) — Theory and application.
  **Day 37**: Naive Bayes Classifier — Theory and ideal use cases.
  **Day 38**: Regularization in Regression — L1, L2 regularization, and Ridge/Lasso.
  **Day 39**: Polynomial Regression — Handling non-linear relationships.
  **Day 40**: Decision Tree Pruning — Avoiding overfitting.
  **Day 41**: Ensemble Learning Practical — Stacking and voting classifiers.
  **Day 42**: Model Interpretability — Tools like SHAP and LIME.

#### **Week 7–8: Neural Networks and Deep Learning Basics**
  **Day 43**: Intro to Neural Networks — Biological vs. artificial networks.
  **Day 44**: Feedforward Neural Networks — Theory and components.
  **Day 45**: Backpropagation and Optimization — Understanding backpropagation and optimizers.
  **Day 46**: Activation Functions — Sigmoid, ReLU, tanh, and softmax.
  **Day 47**: Loss Functions — Cross-entropy, MSE, and MAE.
  **Day 48**: Training Neural Networks — Hyperparameters, batch size, epochs.
  **Day 49**: Overfitting and Underfitting in DL — Regularization techniques.
  **Day 50**: Intro to CNNs — Image processing basics.
  **Day 51**: CNN Architecture — Pooling, padding, and strides.
  **Day 52**: CNN Applications — Image classification, object detection.
  **Day 53**: Recurrent Neural Networks (RNNs) — Sequence modeling.
  **Day 54**: RNN Applications — Text processing, time-series forecasting.

#### **Week 9–10: Advanced Topics and Specialized Models**
  **Day 55**: Transfer Learning — Using pre-trained models.
  **Day 56**: Autoencoders — Dimensionality reduction and anomaly detection.
  **Day 57**: Generative Adversarial Networks (GANs) — Generating synthetic data.
  **Day 58**: Attention Mechanisms — Foundation of Transformer models.
  **Day 59**: Intro to Transformers — BERT, GPT basics.
  **Day 60**: Self-Supervised Learning — Learning without labels.
  **Day 61**: Active Learning — Labeling data efficiently.
  **Day 62**: Reinforcement Learning Basics — Agent, environment, rewards.
  **Day 63**: Policy-Based RL Models — Q-learning, Deep Q-Networks.
  **Day 64**: Reinforcement Learning Applications — Gaming, robotics.

#### **Week 11–12: Deploying ML Models**
  **Day 65**: ML Pipeline — Structuring an end-to-end ML project.
  **Day 66**: Data Versioning and Management — DVC, MLflow.
  **Day 67**: Model Serving and APIs — Using Flask or FastAPI.
  **Day 68**: ML Model Deployment — Deploying on AWS, Azure, GCP.
  **Day 69**: Monitoring Models in Production — Concept drift, retraining.
  **Day 70**: Explainability in Production — Importance of interpretability.

#### **Week 13–14: Real-World Projects and Applications**
  **Day 71–100**: Project Phase — Tackle real-world projects and finalize along the way.

*Looking forward to diving into this journey with you all, one day at a time!*
